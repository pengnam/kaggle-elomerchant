{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elo Merchant\n",
    "\n",
    "## Introduction\n",
    "This notebook represents Sean Ng's submission to elo merchant. \n",
    "\n",
    "I got some feature engineering ideas from:\n",
    "\n",
    "https://www.kaggle.com/denzo123/a-closer-look-at-date-variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions to manage memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To reset all variables\n",
    "def remove_var(*varnames):\n",
    "    \"\"\"\n",
    "    varnames are what you want to keep\n",
    "    \"\"\"\n",
    "    globals_ = globals()\n",
    "    to_save = {v: globals_[v] for v in globals_ if v not in varnames }\n",
    "    to_save['my_reset'] = my_reset  # lets keep this function by default\n",
    "    del globals_\n",
    "    get_ipython().magic(\"reset\")\n",
    "    globals().update(to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_variables():\n",
    "    import sys\n",
    "\n",
    "    # These are the usual ipython objects, including this one you are creating\n",
    "    ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "\n",
    "    # Get a sorted list of the objects and their sizes\n",
    "    sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Declaring imports\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n",
    "                              GradientBoostingClassifier, ExtraTreesClassifier)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "import xgboost as xgd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPLORE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Files\n",
    "train_data_path = '../input/train.csv'\n",
    "test_data_path = '../input/test.csv'\n",
    "history_path = '../input/historical_transactions.csv'\n",
    "merchant_path = '../input/merchants.csv'\n",
    "new_transactions_path = '../input/new_merchant_transactions.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to  4.04 Mb (56.2% reduction)\n",
      "Mem. usage decreased to  2.24 Mb (52.5% reduction)\n"
     ]
    }
   ],
   "source": [
    "train_df = reduce_mem_usage(pd.read_csv(train_data_path, parse_dates=[\"first_active_month\"]))\n",
    "test_df = reduce_mem_usage(pd.read_csv(test_data_path, parse_dates=[\"first_active_month\"]))\n",
    "n_train = train_df.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchants = pd.read_csv(merchant_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 1867.06 Mb (43.7% reduction)\n"
     ]
    }
   ],
   "source": [
    "all_transactions = reduce_mem_usage(\n",
    "    pd.read_csv(new_transactions_path, parse_dates=[\"purchase_date\"])\n",
    "    .append(\n",
    "        pd.read_csv(history_path, parse_dates=[\"purchase_date\"]),\n",
    "        ignore_index=True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/tensorflow/lib/python3.6/site-packages/pandas/core/frame.py:6211: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    }
   ],
   "source": [
    "all_data = train_df.append(test_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>325540.00000</td>\n",
       "      <td>325540.000000</td>\n",
       "      <td>325540.000000</td>\n",
       "      <td>201917.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.10681</td>\n",
       "      <td>1.744038</td>\n",
       "      <td>0.565116</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.18728</td>\n",
       "      <td>0.750540</td>\n",
       "      <td>0.495742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-33.218750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.883301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.023438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.765625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>17.968750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          feature_1      feature_2      feature_3         target\n",
       "count  325540.00000  325540.000000  325540.000000  201917.000000\n",
       "mean        3.10681       1.744038       0.565116            NaN\n",
       "std         1.18728       0.750540       0.495742       0.000000\n",
       "min         1.00000       1.000000       0.000000     -33.218750\n",
       "25%         2.00000       1.000000       0.000000      -0.883301\n",
       "50%         3.00000       2.000000       1.000000      -0.023438\n",
       "75%         4.00000       2.000000       1.000000       0.765625\n",
       "max         5.00000       3.000000       1.000000      17.968750"
      ]
     },
     "execution_count": 552,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPLORE:\n",
    "    all_transactions.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data shape:(325540, 6)\n",
      "-----------------------------------------------------\n",
      "Train data shape:(201917, 6)\n",
      "Test data shape:(123623, 5)\n",
      "=====================================================\n",
      "All transactions data shape:(31075392, 14)\n",
      "-----------------------------------------------------\n",
      "Old transactions data shape:(29112361, 14)\n",
      "New transactions data shape:(31075392, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"All data shape:\" + str(all_data.shape))\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(\"Train data shape:\" + str(train_df.shape))\n",
    "print(\"Test data shape:\" + str(test_df.shape))\n",
    "print(\"=====================================================\")\n",
    "print(\"All transactions data shape:\" + str(all_transactions.shape))\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(\"Old transactions data shape:\" + str(history_df.shape))\n",
    "print(\"New transactions data shape:\" + str(new_transactions_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_name(df):\n",
    "    for x in globals():\n",
    "        if globals()[x] is df:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_data</th>\n",
       "      <th>all_transactions</th>\n",
       "      <th>merchants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>avg_sales_lag12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_sales_lag3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_sales_lag6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category_2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2764609.0</td>\n",
       "      <td>11887.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category_3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>234081.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first_active_month</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>merchant_id</th>\n",
       "      <td>NaN</td>\n",
       "      <td>164697.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>123623.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    all_data  all_transactions  merchants\n",
       "avg_sales_lag12          NaN               NaN       13.0\n",
       "avg_sales_lag3           NaN               NaN       13.0\n",
       "avg_sales_lag6           NaN               NaN       13.0\n",
       "category_2               NaN         2764609.0    11887.0\n",
       "category_3               NaN          234081.0        NaN\n",
       "first_active_month       1.0               NaN        NaN\n",
       "merchant_id              NaN          164697.0        NaN\n",
       "target              123623.0               NaN        NaN"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding number of null values\n",
    "def count_nulls(df_list):\n",
    "    result = {}\n",
    "    for df in df_list:\n",
    "        vals = {}\n",
    "        for key in df.keys():\n",
    "            count = df[key].isnull().sum()\n",
    "            if count > 0:\n",
    "                vals[key] = count\n",
    "        name = get_df_name(df)\n",
    "        result[name] = vals\n",
    "    return pd.DataFrame.from_dict(result, dtype=int)\n",
    "count_nulls([all_data, all_transactions, merchants])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Viewing the distribution of non-null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "#putting each col into a bucket\n",
    "def classify_categories(df, cols):\n",
    "    discrete = []\n",
    "    continuous = []\n",
    "    for col in cols:\n",
    "        \n",
    "        length = len(df[col].unique())\n",
    "        if length <= 25 :\n",
    "            discrete.append(col)\n",
    "        else:\n",
    "            continuous.append(col)\n",
    "    return discrete, continuous\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_na(df, cols):\n",
    "    discrete, continuous = classify_categories(df, cols)\n",
    "    all_count = len(continuous) + len(discrete)\n",
    "    #Distributions of continuous dVata\n",
    "    fig, axes = plt.subplots(nrows=all_count, ncols=1, figsize=(3,5*all_count))\n",
    "    if all_count == 1:\n",
    "        axes = [axes]\n",
    "    for i, col_name in enumerate(discrete):\n",
    "        df[col_name].value_counts().plot(kind='bar', ax=axes[i], title = col_name)\n",
    "    for i, col_name in enumerate(continuous):\n",
    "        df[col_name].plot(kind='density', ax=axes[i+len(discrete)], title = col_name)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPLORE:\n",
    "    plot_na(all_transactions, [\"category_2\", \"category_3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPLORE:\n",
    "    plot_na(merchants, ['avg_sales_lag12','avg_sales_lag3','avg_sales_lag6','category_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPLORE:\n",
    "    #Plot date to get a rough idea\n",
    "    all_data.set_index(['first_active_month']).groupby('first_active_month').card_id.count().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPLORE:\n",
    "    merchants.avg_sales_lag3[merchants.avg_sales_lag3!=np.nan].sort_values(ascending=False)[500:].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting date to corresponding date difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "from dateutil import relativedelta\n",
    "def convert_date_to_timediff(df, cols):\n",
    "    today = datetime.today()\n",
    "    for col in cols:\n",
    "        diff = df[col].dropna().map(lambda x: relativedelta.relativedelta(today,x))\n",
    "        diff = diff.map(lambda x: x.years * 12 + x.months).astype('int64')\n",
    "        df[col] = diff\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_data = convert_date_to_timediff(all_data, [\"first_active_month\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_transactions = convert_date_to_timediff(all_transactions, [\"purchase_date\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the day of week relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Cython extension is already loaded. To reload it, use:\n",
      "  %reload_ext Cython\n"
     ]
    }
   ],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "def f(x):\n",
    "    return x.weekday()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_transactions[\"day_of_week\"] = all_transactions[\"purchase_date\"].apply(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperating data by flag because apparently they have significant difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = all_transactions[\"authorized_flag\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LabelEncode/ Hot Encode the necessary values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "def LabelEncodeCols(df, cols):\n",
    "    new_df = df.copy()\n",
    "    for col in cols:\n",
    "        lbl = LabelEncoder()\n",
    "        new_df[col] = lbl.fit_transform(new_df[col])\n",
    "    return new_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_transactions = LabelEncodeCols(all_transactions, [\"authorized_flag\", \"category_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_transactions = pd.get_dummies(all_transactions, columns=[\"category_2\", \"category_3\", \"day_of_week\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28558483, 27)"
      ]
     },
     "execution_count": 573,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_transactions_flagged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31075392, 27)"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_transactions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_transactions_flagged = all_transactions[flag=='Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_transactions = all_transactions[flag=='N']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Consider if I should flag out data that is not authoized (authorized flag)\n",
    "def aggregate(history):\n",
    "    \n",
    "    agg = {\n",
    "        \"authorized_flag\":[\"mean\", \"var\"],\n",
    "        \"category_1\":[\"mean\", \"var\"],\n",
    "        \"category_2_1.0\":[\"mean\"], \n",
    "        \"category_2_2.0\":[\"mean\"],\n",
    "        \"category_2_3.0\":[\"mean\"],\n",
    "        \"category_2_4.0\":[\"mean\"],\n",
    "        \"category_2_5.0\":[\"mean\"],\n",
    "        \"category_3_A\":[\"mean\"],\n",
    "        \"category_3_B\":[\"mean\"],\n",
    "        \"category_3_C\":[\"mean\"],\n",
    "        \"city_id\":['nunique'],\n",
    "        \"month_lag\":[np.ptp, \"mean\"],\n",
    "        \"installments\":[\"max\", \"mean\", \"var\"],\n",
    "        \"merchant_id\":['nunique'],\n",
    "        \"state_id\":['nunique'],\n",
    "        \"subsector_id\":[\"nunique\"],\n",
    "        \"purchase_amount\":[\"mean\", \"std\", np.ptp],\n",
    "        \"day_of_week_0\":[\"mean\"],\n",
    "        \"day_of_week_1\":[\"mean\"],\n",
    "        \"day_of_week_2\":[\"mean\"],\n",
    "        \"day_of_week_3\":[\"mean\"],\n",
    "        \"day_of_week_4\":[\"mean\"],\n",
    "        \"day_of_week_5\":[\"mean\"],\n",
    "        \"day_of_week_6\":[\"mean\"],\n",
    "        \"card_id\":[\"size\"]\n",
    "    }\n",
    "    \n",
    "    new_df = history.groupby(\"card_id\").agg(agg)\n",
    "    #Replace columns\n",
    "    new_df.columns = [' '.join(col).strip() for col in new_df.columns.values]\n",
    "    return new_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_transactions = aggregate(all_transactions)\n",
    "aggregate_flagged_transactions = aggregate(all_transactions_flagged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_flagged_transactions.columns = [\"flag_\" + x for x in aggregate_flagged_transactions.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.merge(all_data, aggregate_transactions, on='card_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.merge(all_data, aggregate_flagged_transactions, on='card_id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consider adding other columns. i.e. grouping the values across time (!!!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuning hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features that others added\n",
    "#Handling month_lag?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Month difference between today and purchase date (might be more significant than just date(?). I need to be able to convert to something significant anyway\n",
    "2. Aggregation based statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = all_data['first_active_month']\n",
    "del all_data['first_active_month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 46.88 Mb (69.6% reduction)\n"
     ]
    }
   ],
   "source": [
    "all_data = reduce_mem_usage(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"target\" in all_data.keys():\n",
    "    target = all_data[\"target\"][:n_train]\n",
    "if \"card_id\" in all_data.keys():\n",
    "    card_ids = all_data[\"card_id\"]\n",
    "to_be_deleted = [\"target\", \"card_id\"]\n",
    "for col in to_be_deleted:\n",
    "    if col in all_data.keys():\n",
    "        del all_data[col]\n",
    "train = all_data[:n_train]\n",
    "test = all_data[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the right features\n",
    "param = {'num_leaves': 111,\n",
    "         'min_data_in_leaf': 149, \n",
    "         'objective':'regression',\n",
    "         'max_depth': 9,\n",
    "         'learning_rate': 0.005,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.7522,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.7083 ,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'rmse',\n",
    "         \"lambda_l1\": 0.2634,\n",
    "         \"random_state\": 133,\n",
    "         \"verbosity\": -1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "def run_lgbm_cv():\n",
    "    categorical_feats = [\"feature_2\", \"feature_3\"]\n",
    "    folds = KFold(n_splits = 5, shuffle=True, random_state=15)\n",
    "    oof = np.zeros(len(train))\n",
    "    predictions = np.zeros(len(test))\n",
    "    start = time.time()\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    features = all_data.keys()\n",
    "\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n",
    "        print(\"fold n°{}\".format(fold_))\n",
    "        trn_data = lgb.Dataset(train.iloc[trn_idx][features],\n",
    "                               label=target.iloc[trn_idx],\n",
    "                               categorical_feature=categorical_feats\n",
    "                              )\n",
    "        val_data = lgb.Dataset(train.iloc[val_idx][features],\n",
    "                               label=target.iloc[val_idx],\n",
    "                               categorical_feature=categorical_feats\n",
    "                              )\n",
    "\n",
    "        num_round = 10000\n",
    "        clf = lgb.train(param,\n",
    "                        trn_data,\n",
    "                        num_round,\n",
    "                        valid_sets = [trn_data, val_data],\n",
    "                        verbose_eval=100,\n",
    "                        early_stopping_rounds = 200)\n",
    "\n",
    "        oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
    "\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = features\n",
    "        fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "        fold_importance_df[\"fold\"] = fold_ + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "\n",
    "        predictions += clf.predict(test[features], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "    print(\"CV score: {:<8.5f}\".format(mean_squared_error(oof, target)**0.5))\n",
    "    return feature_importance_df, predictions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "feature_importance_df, predictions = run_lgbm_cv()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "feature_importances = feature_importance_df.groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "feature_importances = feature_importances.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPLORE:\n",
    "    plt.figure(figsize=(14,25))\n",
    "    plt.title('LightGBM feature importances')\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.DataFrame({\"card_id\":test_df.card_id.values})\n",
    "sub_df[\"target\"] = predictions\n",
    "sub_df.to_csv(\"submit.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPLORE:\n",
    "    corrmat = all_data.corr()\n",
    "    plt.subplots(figsize=(12,9))\n",
    "    sns.heatmap(corrmat, vmax=0.9, square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds=5\n",
    "def rmsle_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n",
    "    rmse= np.sqrt(-cross_val_score(model, train.values, target, scoring=\"neg_mean_squared_error\", cv = kf, verbose=1))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, Lasso, Ridge, BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.001, random_state=1, tol=0.3))\n",
    "\n",
    "ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.5, l1_ratio=.9, random_state=3))\n",
    "ridge = make_pipeline(RobustScaler(), Ridge(alpha=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = rmsle_cv(lasso)\n",
    "print(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = rmsle_cv(ENet)\n",
    "print(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = rmsle_cv(ridge)\n",
    "print(\"Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Doesn't seem to work\n",
    "KRR =  KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "score = rmsle_cv(KRR)\n",
    "print(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting score: 3.8617 (0.0358)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.2min finished\n"
     ]
    }
   ],
   "source": [
    "GBoost = GradientBoostingRegressor(n_estimators=100, learning_rate=0.05,\n",
    "                                   max_depth=3, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)\n",
    "\n",
    "score = rmsle_cv(GBoost)\n",
    "print(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgboost score: 3.8381 (0.0339)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.3min finished\n"
     ]
    }
   ],
   "source": [
    "model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                             learning_rate=0.05, max_depth=3, \n",
    "                             n_estimators=100,\n",
    "                             random_state =7, nthread = -1)\n",
    "\n",
    "score = rmsle_cv(model_xgb)\n",
    "print(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] ...................... , score=-14.596460013018843, total=   3.5s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...................... , score=-14.641113285954846, total=   3.2s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    6.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...................... , score=-14.518480013374495, total=   3.2s\n",
      "[CV]  ................................................................\n",
      "[CV] ...................... , score=-14.651496713617208, total=   3.3s\n",
      "[CV]  ................................................................\n",
      "[CV] ...................... , score=-15.217078129992197, total=   3.2s\n",
      "LGBM score: 3.8372 (0.0325)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   16.5s finished\n"
     ]
    }
   ],
   "source": [
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "                              learning_rate=0.05, n_estimators=720,\n",
    "                              max_bin = 55, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n",
    "score = rmsle_cv(model_lgb)\n",
    "print(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Optimizing variable names\n",
    "dtrain = xgb.DMatrix(train.values, label = target)\n",
    "params = {\"max_depth\":3, \"eta\":0.1}\n",
    "model = xgb.cv(params, dtrain, num_boost_round = 500, early_stopping_rounds=100)\n",
    "model.loc[:,[\"train-rmse-mean\", \"test-rmse-mean\"]].plot()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.loc[:,[\"train-rmse-mean\", \"test-rmse-mean\"]].plot()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "def get_best_params(rounds = 1, model, grid):\n",
    "    for i in range(rounds):\n",
    "        gsearch = GridSearchCV(estimator=xgb.XGBRegressor(learning_rate = 0.1), param_grid = {'max_depth':list(range(3,5,1)), 'n_estimators':[100,110,120]}, n_jobs=2, iid=False, cv=2)\n",
    "        gsearch.fit(train.values, target)\n",
    "    return gsearch.best_params_\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "params = get_best_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 3, 'n_estimators': 100}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
